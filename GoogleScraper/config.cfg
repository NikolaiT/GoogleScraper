; Configuration parameters that control the scraping process. You will most
; likely want to change these values.
[SCRAPING]
; The search queries to search for, separated by newlines. Intend every new
; keyword-line at least more than the next keyword.
; Example:

; keywords:  Apples
;           Peaches
;           Bananas
;           Cheapest Hotel in California
;           inurl: "admin.php"

keywords:

; The keyword file. If this is a valid file path, the keywords params will be ignored and
; the ones from the file will be taken. Each keyword must be on a separate line.
keyword_file:

; How many results per SERP page
num_results_per_page: 10

; How many pages should be requested for each single keyword
num_pages_for_keyword: 1

; This arguments sets the number of browser instances for selenium mode or the number of worker threads in http mode.
num_workers: 1

; Maximum of workers
; When scraping with multiple search engines and more than one worker, the number of total workers
; becomes quite high very fast, so we set a upper limit here. Leaving this out, is quite dangerous in selenium mode.
maximum_workers: 20

; The search offset on which page to start scraping.
; Pages begin at 1
search_offset: 1

; In some countries the main search engine domain is blocked. Thus, search engines
; have different ip on which they are reachable. If you set a file with urls for the search engine,
; then GoogleScraper will pick a random url for any scraper instance.
; One url per line. It needs to be a valid url, not just an ip address!
; Example: google_ip_file: google_ips.txt

google_ip_file:

; List of supported search engines
; If you add support for another search engine (of course implement it in the
; appropriate places) add it in this list.
; , blekko
supported_search_engines: google, yandex, bing, yahoo, baidu, duckduckgo, ask

; The search engine to use. For the supported search engines, see above "supported_search_engines"
; If you want to use more than one, just append with commas
; like that: "google, yandex, bing, duckduckgo"
search_engines: google

;;; The base search urls 
;;; Ready to append the parameters at the end to fine tune the search.

; The google base search url
google_search_url: https://www.google.com/search?

; The yandex base search url
yandex_search_url: http://yandex.ru/yandsearch?

; The bing base search url
bing_search_url: http://www.bing.com/search?

; The yahoo base search url
yahoo_search_url: https://de.search.yahoo.com/search?

; The baidu base search url
baidu_search_url: http://www.baidu.com/s?

; The duckduckgo base search url
duckduckgo_search_url: https://duckduckgo.com/

; The ask base search url
ask_search_url: http://de.ask.com/web?

; The blekko base search url
blekko_search_url: http://blekko.com/#ws/?

; The search type. Currently, the following search modes are
; supported for some search engine: normal, video, news and image search.
; "normal" search type is supported in all search engines.
search_type: normal

; The scrape method. Can be 'http' or  'selenium' or 'http-async'
; http mode uses http packets directly, whereas selenium mode uses a real browser (or phantomjs).
; http_async uses asyncio.
scrape_method: selenium

; If scraping with the own IP address should be allowed.
; If this is set to False and you don't specify any proxies,
; GoogleScraper cannot run.

use_own_ip: True

; Whether to check proxies before starting the scrape
check_proxies: True

; You can set the internal behaviour of GoogleScraper here
; When GoogleScraper is invoked as a command line script, it is very much desirable
; to be as robust as possible. But when used from another program, we need immediate
; response when something fails.
raise_exceptions_while_scraping: False

; Global configuration parameters that apply on all modes.
[GLOBAL]
; The proxy file. If this is a valid file path, each line will represent a proxy.
; Example file:
;        socks5 23.212.45.13:1080 username:password
;        socks4 23.212.45.13:80 username:password
;        http 23.212.45.13:80
proxy_file:


; Whether to continue the last scrape
continue_last_scrape: True

; Proxies stored in a MySQL database. If you set a parameter here, GoogleScraper will look for proxies
; in a table named 'proxies' for proxies with the following format:
;
; CREATE TABLE proxies (
;   id INTEGER PRIMARY KEY NOT NULL,
;   host VARCHAR(255) NOT NULL,
;   port SMALLINT,
;   username VARCHAR(255),
;   password VARCHAR(255),
;   protocol ENUM('socks5', 'socks4', 'http')
; );
;
; Specify the connection details in the following format: mysql://<username>:<password>@<host>/<dbname>
; Example: mysql://root:soemshittypass@localhost/supercoolproxies
mysql_proxy_db:

; Whether to manually clean cache files. For development purposes
clean_cache_files: False

; Proxy checker url
proxy_check_url: http://canihazip.com/s

; Proxy info url
proxy_info_url: http://ipinfo.io/json

; Set the debug level of the application. Must be an integer.
; CRITICAL = 50
; FATAL = CRITICAL
; ERROR = 40
; WARNING = 30
; WARN = WARNING
; INFO = 20
; DEBUG = 10
; NOTSET = 0
debug: INFO

; extended configuration from command line. Setting this here is pointless, since
; it only modifies values that can be set here. Inception?
; No just bad design by me.
extended_config:

; The verbosity level
; There are different levels of verbosity:
; - 0 will output nothing at all.
; - 1 will show the most necessary information while scraping (doesn't include results)
;     like the creation of new scraper instances (threads) or the logging of important events.
;     Shows also the progress.
; - 2 will show detailed information while scraping (still without showing parsed results)
;     like the happening of every new request of SERP page.
; - 3 shows additionally the SERP results with all links.
; - 4 will show quite detailed debug messages
;     for example all database interaction events from sqlalchemy.
; - 5 leaks all internal functioning of the app
verbosity: 1

; The basic search url
base_search_url: http://www.google.com/search

; Whether caching shall be enabled
do_caching: True

; Whether the whole html files should be cached or
; if the file should be stripped from unnecessary data like javascripts, comments, ...
minimize_caching_files: True

; If set, then compress/decompress cached files
compress_cached_files: True

; Use either bz2 or gz to compress cached files
compressing_algorithm: gz

; The relative path to the cache directory
cachedir: .scrapecache/

; After how many hours should the cache be cleaned
clean_cache_after: 48

; Sleeping ranges.
; The scraper in selenium mode makes random modes every N seconds as specified in the given intervals.
; Format: [Every Nth second when to sleep]; ([Start range], [End range])
sleeping_ranges: 1: 1, 2
                 5: 2, 4
                 30: 10, 20
                 127: 30, 50

; Search engine specific sleeping ranges
; If you add the name of the search engine before a
; option {search_engine_name}_sleeping_ranges, then
; only this search engine will sleep the supplied ranges.

google_sleeping_ranges: 1: 2, 3
                        5: 3, 5
                        30: 10, 20
                        127: 30, 50

; If the search should be simulated instead of being done.
; Useful to learn about the quantity of keywords to scrape and such.
; Won't fire any requests.
simulate: False

; Internal use only
fix_cache_names: False

; All settings that only apply for requesting with real browsers.
[SELENIUM]
; which browser to use in selenium mode. Valid values: ('Chrome', 'Firefox', 'Phantomjs')
sel_browser: Chrome

; Manual captcha solving
; If this parameter is set to a Integer, the browser waits for the user
; to enter the captcha manually whenever Google detected the script as malicious.
;
; Set to False to disable.
; If the captcha isn't solved in the specified time interval, the browser instance
; with the current proxy is discarded.
manual_captcha_solving: False

; All settings that target the raw http packet scraping mode.
[HTTP]

;; You may overwrite the global search urls in the SCRAPING section
;; for each mode
;; search engine urls for the specific engines
; The google search url specifiably for http mode
google_search_url: https://www.google.com/search?


[HTTP_ASYNC]
; The number of concurrent requests that are used for scraping
max_concurrent_requests: 100

[PROXY_POLICY]

; How long to sleep (in seconds) when the proxy got detected.
proxy_detected_timeout: 400

; Whether to stop workers when they got detected instead of waiting.
stop_on_detection: True

[OUTPUT]

; The name of the database
database_name: google_scraper

; The file name of the output
; The file name also determine the format of how
; to store the results.
; filename.json => save results as json
; filename.csv => save a csv file
; If set to None, don't write any file.
output_filename: None
